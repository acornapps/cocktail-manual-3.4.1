---
title: "NFS HA(High Availability) 구성"
excerpt: ""
permalink: /docs/ko/8.1.8/
redirect_from:
  - /theme-setup/
toc: false
toc_sticky: false
---

# Overview

Google Cloud 환경에서 Storage High Availibity를 위해 NFS server를 pacemaker, corosync, drbd를 이용하여 이중화하는 구성을 설명합니다. 
 - Corosync: 클러스터의 저수준 인프라를 제공하며, 클러스터에 대한 쿼럼정보, 멤버쉽, 안정적인 메시징을 제공.
 - Pacemaker: 클러스터의 리소스 관리자. 클러스터 상태에 따른 자원이동, 노드 정지등을 담당함.
 - DRBD(Distributed Replicated Block Device): 호스트간 네트웍을 이용하여 소프트웨어적으로 block device의 content를 replicate하는 기능을 제공함.

----

# Installation

### Google cloud VM instance 생성
* 인스턴스 템플릿을 생성 합니다. GCP 인스턴스 그룹을 생성 합니다.
* Load balancer 생성 한다. (ex: 10.174.0.4 )
* Load balancer 설정 한다. 
    - Backend configuration  : 1번에서 생성한 인스턴스 그룹 선택 후 2049 포트 health check
    - Frontend configuration : PORT 설정 all
    - 방화벽 설정한다.

### NFS 1,2 서버에 DRBM 설치
* DRBD package 설치

```
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
yum install -y kmod-drbd84 drbd84-utils
```

* 방화벽 off

```
systemctl stop firewalld
systemctl disable firewalld
```

* drbd configuration file 설정

``` 
mv /etc/drbd.d/global_common.conf /etc/drbd.d/global_common.conf.orig
vi /etc/drbd.d/global_common.conf 

global {
 usage-count  yes;
}

common {
 net {
  protocol C;
 }
}
```

* lvm2 설치 pv 설정 및 리소스 설정

```
yum install -y lvm2 telnet

pvcreate /dev/sdb
vgcreate drbd_vg /dev/sdb
lvcreate -l 100%FREE -n drbd_lv drbd_vg
```

* 아래 conf 설정 시 gcp는 hostname 으로 변경 hostname 또는 아이피 변경

```
vi /etc/drbd.d/r0.res
resource r0 {
    handlers {
        split-brain "/usr/lib/drbd/notify-split-brain.sh root"; // alarm noti를 위한 script file 설정
    }
    on nfs-master {
        device /dev/drbd0;
        disk /dev/drbd_vg/drbd_lv;
        meta-disk internal;
        address 10.128.0.18:7789;
    }
    on nfs-slave  {
        device /dev/drbd0;
        disk /dev/drbd_vg/drbd_lv;
        meta-disk internal;
        address 10.128.0.19:7789;
    }
}
```
 
* drbd 리소스 생성
```
drbdadm create-md r0
```

* drbd 리소스 시작
```
drbdadm up r0
```

* 마스터 승격 및 디바이스 설정
```
drbdadm primary --force r0
mkfs.xfs /dev/drbd0
```

* 시스템 enable
```
systemctl enable drbd 
```

* NFS 1번 서버 테스트
```bash
mount /dev/drbd0 /mnt
touch /mnt/test-01.txt
ls /mnt
umount /mnt
```

* NFS 2번 서버 테스트    
```bash
// 1번에서 primary 를 secondary 설정
drbdadm secondary r0
```
 
* NFS 2번에서 secondary 를 primary  설정    
```bash
drbdadm primary r0
mount /dev/drbd0 /mnt
ls /mnt
umount /mnt
drbdadm secondary r0
```
 
* 현재 상태를 확인 할수 있음
  - 블록 디바이스의 싱크 정보를 확인 할수 있다.
    ```    
    watch more /proc/drbd
    ```    
  - pacemaker 상태 확인
    ```    
    watch pcs status
    ```    

---

### NFS 1,2 서버에 pacemaker, corosync, pcsd 설치
* pacemaker 관련 모듈 설치

```bash
yum install -y pacemaker pcs psmisc policycoreutils-python 

passwd hacluster
systemctl start pcsd

setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

vi /etc/exports
// 접속 대역(CIDR) 정의 필요함. 
/storage 10.128.0.0/24(rw,async,no_root_squash,no_all_squash,no_subtree_check)
```

* 1번(Master 서버에서 실행)

```bash
pcs cluster auth storage1 storage2
hacluster 
 
pcs cluster setup --start --name nfs-cluster storage1 storage2 --force

// drdb 컨트롤 설정 부분 
pcs cluster cib drbd_cfg
pcs -f drbd_cfg resource create Data ocf:linbit:drbd drbd_resource=r0 op monitor timeout="10" interval="10" role="Slave" op monitor timeout="10" interval="5" role="Master"
pcs -f drbd_cfg resource create Storage Filesystem device="/dev/drbd0" directory="/storage" fstype="xfs" op monitor interval="30s"
pcs -f drbd_cfg resource master DataSync Data master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true 
pcs -f drbd_cfg resource master DataSync NFS-Server master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true 
pcs -f drbd_cfg resource create NFS-Server systemd:nfs-server op monitor interval="40s"
pcs -f drbd_cfg resource group add HA-GROUP Storage  NFS-Server 
pcs -f drbd_cfg constraint colocation add HA-GROUP DataSync INFINITY with-rsc-role="Master"
pcs -f drbd_cfg constraint colocation add HA-GROUP NFS-Server INFINITY with-rsc-role="Master"
pcs -f drbd_cfg constraint order promote DataSync then start Storage
pcs -f drbd_cfg constraint order Storage then start NFS-Server 
pcs cluster cib-push drbd_cfg

//활성화
pcs property set stonith-enabled=false
```

* 기본으로 pacemaker cluster heartbeat timeout은 1초로 되어 있으므로 10초로 아래와 같이 늘려준다.

```bash
# cat /etc/corosync/corosync.conf
totem {
    version: 2
    cluster_name: pcmk
    secauth: off
    transport: udpu
    token: 10000     <---- change totem timeout to 10s.
}

pcs cluster sync            // sync config file to all node
pcs cluster reload corosync // take effect all nodes without downtime
```

* 각 NFS server에서 자동 기동 설정
```bash
systemctl enable corosync
systemctl enable pcsd
systemctl enable pacemaker
```

* pacemaker 기동/중지
```bash
systemctl start pcsd    // NFS1, NFS2번에 동일하게 시작
systemctl cluster [start | stop] // 한쪽 장비에서 기동 또는 종료
```

* pcsd web UI

https://{nfs server public ip}:2224

---

### 사용중에 NFS storage disk size 늘리기
* GCE DISK 관리 화면에서 NFS 서버에 연결되어 있는 1,2 디스크를 동일한 크기 만큼 늘려 준다. (ex: 50G -> 200G)
* NFS 서버 에서 물리적인 디스크가 늘어났는지 확인
* DISK 논리 적용

```bash
partprobe
pvresize /dev/sdb
```

* 확인 1
```bash
pvdisplay
```
 
* 확인 2
```bash
lvscan
```

* 디스크 확장 이전 상태 , 아래로 대체 가능
```bash
lvchange -a y /dev/drbd_vg/drbd_lv
lvresize -L +150G /dev/drbd_vg/drbd_lv
```

* 디스크 확장 변경 사항
```bash
lvextend -l +100%FREE /dev/drbd_vg/drbd_lv
```

* 디스크 확장 후 적용을 해줘야 df -h 가 적용 된다.
```bash
xfs_growfs /dev/drbd0
lvscan
```

* 마스터서버에서 싱크 및 논리적인 디스크 적용
```bash
drbdadm resize r0
```

---
# Operations
### 1. DRBD 상태 조회하기
```bash
drbdsetup show --show r0                    // 구성 정보 조회
drbdadm status r0                           // 상태 조회
drbdsetup  status r0 --verbose --statistic  // 상태 상세 조회
drbdsetup events2 --now r0                  // 실시간 이벤트 조회
drbdadm cstate r0                           // 네트웍 상태 조회
drbdadm role r0                             // 리소스 역할 조회
drbdadm dstate r0                           // 디스크 상태 조회
``` 

### 2. Pacemaker 상태 조회하기
```bash
pcs status                                 // pacemaker cluster 상태 조회
pcs cluster sync                           // 원격 노드와 구성 동기화
pcs cluster reload corosync                // 구성 정보 reload. 순단 현상 없음.
pcs alert config                           // Alert 구성정보 조회
pcs cluster corosync [node]                // corosync.conf 구성 정보 조회

systemctl [start| stop] pcsd                // pcsd daemon start/stop
pcs cluster [start|stop]                    // pacemaker cluster start/stop

``` 

---
# Troubleshooting
### 1. NFS storage DRBD 연결 단절후 양쪽 노드가 Primary 역할이었을 경우 수동으로 복구방법
* DRBD가 발생하는 alarm중에 split brain 이 발생했다는 alarm이 발생할 수 있습니다.
* 이 경우는 NFS server의 네트웍 연결 단절후 양 노드가 Primary 역할이었음을 감지했을 경우 먼저 변경사항을 폐기할 node(victim)를 선택하고
다음 명령을 통해 수동으로 복구합니다.

```bash
drbdadm secondary r0
drbdadm connect --discard-my-data r0
``` 

다른 노드(survivor라 함)에서는 단지 연결 명령만 실행하면 됩니다.

```bash
drbdadm disconnect r0    // 먼저 disconnect를 한후 다시 연결 명령을 수행함.
drbdadm connect r0
```

정상적으로 연결되었다면 Split Brain victim의 연결 상태는 즉시 SyncTarget으로 변경되고, victim 노드의 변경사항은 survivor 노드에 의해 덮어 씌워집니다
아래 명령으로 확인합니다.
```bash
drbdadm status r0
```