---
title: "GKEクラスターの作成"
excerpt: ""
permalink: /docs/ja/8.1.1/
redirect_from:
  - /theme-setup/
toc: true
toc_sticky: false
sidebar:
  nav: "ja"
---

## GKEクラスターを作成する方法を説明します。

### Google Kubernetes Engine(GKE)にログイン

<https://cloud.google.com/kubernetes-engine/> で Google cloud にログインします。

### サービスアカウントの作成

### 1. IAMおよび管理者のサービスアカウントを作成します。

* サービスアカウントには、次の役割が必要です。

    * project/viewer

    * kubernetes-engine/admin

    * service-account/user

    * 役割ベースのアクセス制御(Identity and Access Management)を使用するための必要条件：  
    **RBAC生成時の前提条件の要素が必要**.

        * 次のコマンドを実行して、Kubernetesで役割を作成する権限をユーザに付与する必要があります。 [USER_ACCOUNT]は、ユーザーの電子メールアドレスです。  
        参照: <https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control>

        ```bash
        kubectl create clusterrolebinding cluster-admin-binding \
        --clusterrole cluster-admin --user [USER_ACCOUNT]
        ```

        or

        ```bash
        kubectl create clusterrolebinding cluster-admin-binding \
        --clusterrole cluster-admin \
        --user $(gcloud config get-value account)
        ```


* サービスアカウントの作成を選択します。  
**参照:** [サービスアカウントでCloud Platform認証](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform)

    * サービスアカウントの詳細 

        * **サービスアカウント名:**  
        このサービスアカウントの表示名です.

        * **作成**を選択します。

    * サービスアカウントの権限（オプション）

        * **役割:**  
        この使用説明書では、便宜上、所有者権限を付与します。

    * Private key 作成

        * **CREATE KEY** 選択します。

            * **CREATE** します。  
            サービスアカウントが作成されたら、サービスアカウントのユーザー認証情報が含まれているJSONのキーファイルがコンピュータにダウンロードされます。このキーファイルは、ユーザーのAPIの認証を実行するようにアプリケーションを構成するために使用されます。

    ![gke-create-service-account-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-service-account-1.png)
    ![gke-create-service-account-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-service-account-2.png)
    ![gke-create-service-account-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-service-account-3.png)


### GKEクラスターの作成
**参照:** [クラスターアーキテクチャ](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture)

### 1. プロジェクトの作成
**参照:** <https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster>

* プロジェクトの作成を選択します。[詳細](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

    * **Project Name:**  
    プロジェクトの名前を入力します。

        * **Preject ID**  
        プロジェクトIDは、プロジェクトのグローバルで一意の識別子です。プロジェクトを作成した後は、プロジェクトIDを変更することができません。  
        プロジェクトを作成するとき、またはプロジェクトIDを作成するためのAPIを有効にすると、選択したカスタム設定された名前になります。削除されたプロジェクトのプロジェクトIDは再利用することはできません。

    * **Location:**  
    親組織またはフォルダを選択します。

    ![gke-create-project-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-project-1-1.png)


### 1.1 Google Kubernetes Engine APIが有効かどうかを確認します。

    **GOOGLE KUBERNETES ENGINEAPI 利用設定** を確認します。

    ![gke-create-api-lib-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-api-lib-1.png)

* [Cloud SDK](https://cloud.google.com/sdk/downloads?hl=ko)がインストールされていることを確認します。

    * gcloud コマンドラインツールのデフォルト値を設定します。  
    gcloud コマンドラインツールで **プロジェクトID** と **Compute Engine 領域** オプションを入力する時間を節約するためにデフォルト値として設定することができます。

        ```
        gcloud config set project [PROJECT_ID]
        gcloud config set compute/zone us-central1-b
        ```

    * gcloudを最新バージョンにアップデートします。

        ```
        gcloud components update
        ```

### 2. VPCネットワーク作成

* VPC ネットワーク選択

    * **名前:**  
    * VPCネットワークの作成を選択 

    * **サブネットの作成**:  
    サブネットを使用すると、Google Cloud内自体プライベートクラウドトポロジを作成することができます。各地域にサブネットを作成するには、「自動」をクリックして、サブネットを直接定義するには、「カスタマイズ」をクリックします。 [詳細](https://cloud.google.com/compute/docs/subnetworks)

        * **名前:**  
            サブネット識別名を付与します。

        * **Region:**  
            ここでは、asia-northeast1（Tokyo）を使用します。  
            [詳細](https://cloud.google.com/compute/docs/regions-zones/regions-zones)  

        * **IPアドレスの範囲:**  
            CIDR表記で表したが、サブネットのアドレス範囲です。標準プライベートVPCネットワークアドレスの範囲（例えば、10.0.0.0/9）を使用して下さい。  
            [詳細](https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips)
        
        * **完了**を選択します。

        * 上記のような方法でsubnetを追加します。

    * *作成*を選択します。

    ![gke-create-vpc-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-vpc-1.png)
    ![gke-create-vpc-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-vpc-2.png)
    ![gke-create-vpc-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-vpc-3.png)

### 3. クラスターテンプレート
GCPコンソールを使用して、新しいクラスタを作成する場合は、使用可能なすべてのクラスタのテンプレートが表示されます。基本的には標準的なテンプレートが選択されます。

* 以下のテンプレートを使用することができます。

    * **標準クラスター:**  
    Continuous Integration、ウェブ提供、バックエンド用です。追加のカスタマイズが必要な場合や任意のテンプレートを選択するかわからない場合に選択すると、最も適しています。

    * **最初のクラスター:**  
    以下の強力なノードを実行して、自動拡張など、いくつかの高度な機能を使用していない小さなクラスタです。

    * **CPU集中アプリケーション:**  
    そのノードが標準のクラスタよりも強力なマルチコアCPUを提供するクラスタです。

    * **メモリ集中アプリケーション:**  
    そのノードが一般的に強力な多重コアCPUと大容量メモリーを提供するクラスターです。

    * **GPUアクセラレーションコンピューティング:**  
    基本ノードプールより強力なノードで構成されGPU設定ノードプールに追加で含まれているクラスタです。 [自動拡張](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler?hl=ko)は、基本的には使用されません。

    * **高可用性:**  
    クラスタが地域クラスターで構成され、特定の地域の各領域にクラスターマスターが提供されます。自動拡張とメンテナンス期間が使用されます。

* **Kubernetes クラスターを構成する**

    * **標準クラスター**  
    テンプレートを選択するか、ワークロードに適したテンプレートを選択します。

    * 必要に応じてテンプレートをカスタマイズします。次のフィールドは必須です。

        * **名前:**  
            クラスタ名を選択します。プロジェクトと領域内で一意である。

        * **場所の種類:**  
            クラスタ内のすべてのノードが同じ領域にあることを、指定した地域のすべての領域にあることができるかどうかです。

            * **エリア:**  
                場所の種類が領域である場合は、クラスタを作成するCompute Engine [コンピューティング領域](https://cloud.google.com/compute/docs/regions-zones/)です。  
                ここでは、 **asia-northeast1-a**(Tokyo)を使用します。


            * **地域:**  
                基本的にクラスタは、開発者が作成時に指定した単一のコンピューティングの領域にクラスターマスターと、そのノードを作成します。地域クラスターを作成クラスターの可用性と再構成を向上させることができます。  
                [詳細](https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters)
        
        * **マスターバージョン:**  
        kubernetes versionを選択します。

        * **ノードのプール:**  
            ノードプールは、クラスタ内でKubernetesを実行する別のインスタンスのグループです。

            * **ノード数:**  
                クラスタに作成ノードの数です。ノードとリソースの使用可能な [リソース割り当て](https://cloud.google.com/compute/quotas)必要があります(例:ファイアウォールのパス).

            * **マシンタイプ:**  
                インスタンスに使用Compute Engine [マシンの種類](https://cloud.google.com/compute/docs/machine-types)です。各マシンの種類は異なる料金になります。 基本マシンタイプは、n1-standard-1です。マシンタイプ価格情報は、 [マシンの種類価格表](https://cloud.google.com/compute/pricing)を参照してください。

            * 上級設定:

                * **名前:**  
                ノードプール名を付与します。

                * **ブートディスクのサイズ(GB):**  
                ここでは、10GBを使用します。

                * セキュリティ

                    * **サービスアカウント:**  
                    **Compute Engine default service account を選択します。**  
                    VMで実行されるアプリケーションは、サービスアカウントを使用してGoogle Cloud APIを呼び出します。コンソールメニューの権限を使用してサービスアカウントを作成するか、既定のサービスアカウントがあれば、このアカウントを使用します。 [詳細](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform)

                * **保存**を選択します。
            
        * **詳細オプション:**
            * ネットワーク

                * **VPC ネイティブ:**  
                    VPC ネイティブ有効(エイリアス IP 使用) チェックします。

                * **ネットワーク:**  
                    項目で作成したVPCを選択します。

                * **ノードのサブネット:**  
                    項目で作成したsubnetを選択します。

    * **作成**を選択します。

    ![gke-create-cluster-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-1.png)
    ![gke-create-cluster-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-2.png)
    ![gke-create-cluster-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-3.png)
    ![gke-create-cluster-4]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-4.png)

    GCPコンソールでクラスタを作成した後は、そのクラスタとの連携できるようにkubectlを設定する必要があります。 [kubeconfig ファイルの作成](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)を参照してください。

### 4. クラスターへの接続

* **Cloud Shellで実行:**  

    * Kubernetes クラスターリストから生成されたクラスター**接続**を選択します。

    ![gke-create-cluster-connect-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-connect-1.png)

* **kubectlを使用するには:**

    * kubectl 構成が先行されなければなりません。  
    kubectlは Kubernetes Engineで使用されているクラスターの調整システムである Kubernetesを管理するために使用されます。

        * gcloudを使用して kubectlをインストールすることができます。  

            ```
            gcloud components install kubectl
            ```

    * クラスター接続画面で、コマンドラインをコピーしコンソールで実行します。

        ![gke-create-cluster-connect-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-connect-1.png)

    * 次のコマンドで確認することができます。

        ```
        kubectl get svc
        ```
        ![gke-create-cluster-connect-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-connect-2.png)

### NFS Sever作成
Google Compute Engine Persistent Diskを使用して、ネットワークファイルシステム（NFS）サーバーを作成し、コンテナにマウントします。
                      
                      


### 1. Create an instance

* **名前(Name):**  
インスタンス名を付与します。

* **地域(Region):**  
地域は、リソースを実行する特定の地理的な位置です。

* **領域(Zone):**  
領域は、区域内の隔離された場所です。利用可能なコンピューティングリソースとデータを保存し、使用場所を決定します。

* **マシンタイプ(Machine type):**  
맞춤설정을 클릭하여 코어, 메모리, CPU를 선택합니다.

* **ブートディスク(Boot disk):**  
各インスタンスには、ブート用のディスクが必要です。イメージやスナップショットを選択して、新しいブートディスクを作成したり、既存のディスクをインスタンスに接続します。  
ここでは、CentOS7を使用します。

* **IDとAPIアクセス(Identity and API access):**  
作成したサービスアカウントを選択します。
VMで実行されるアプリケーションは、サービスアカウントを使用してGoogle Cloud APIを呼び出します。使用するサービスアカウントと許可するAPIへのアクセスレベルを選択します。 [詳細](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances?hl)

    ![gke-create-instance-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-1.png)

 **Management, security, disks, networking, sole tenancy 拡張セクション**を有効にして、以下の設定をします。
 
* **ディスク(Disks) の設定**

    * Add new diskを選択します。

        * *ノードに必要なディスクの仕様を設定します。

        ![gke-create-instance-disk-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-disk-1.png)

* **ネットワーク設定**  
ネットワークはインスタンスからアクセスできるネットワークトラフィックを決定します。

    * **Network tags:**  
    ネットワークタグを割り当て,特定の VM インスタンスにファイアウォールルールを適用します。

    * **Network interfaces**

        * **Network:**  
        リストから VPC ネットワークで生成した VPC を選択します

        * **Subnetwork:**  
        リストから VPC ネットワークで生成したサブネットを選択します。

        * **Primary internal IP:**  
        インスタンスを再起動しても、内部IPアドレスが変更されませんが、インスタンスを削除して再作成すると、内部IPアドレスが変更されます。  
         '一時的（自動）'を選択して、サブネットワークの範囲のアドレスを割り当てたり、'一時的（カスタム）'を選択して、直接入力してください。  
        **インスタンスを削除して再作成するときにIPアドレスを維持するには、固定の内部IPアドレスを選択するか、作成します。**  
        [詳細](https://cloud.google.com/compute/docs/subnetworks)

            ![gke-create-instance-network-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-network-2.png)

        * done(完了)**を選択します。

* **Create(作成)**を選択します。

### 2. 生成されたインスタンス (ノード) のファイアウォール設定(SSH / NFS).

* インスタンスの画面で作成したインスタンス拡張メニューから **View network details**を選択する。  
または VPC network > Firewall rulesを選択する。

![gke-create-instance-view-network]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-view-network.png)

* **SSHファイアウォールルールを追加する。**

    * **Name:**  
    例) cocktail-test-gke-demo-storage-ssh

    * **Network:**  
    インスタンスで設定したVPCを選択します。

    * **Targets(対象):**  
    **Specified target tags**を選択する。  
    仮想ネットワークでは、インスタンスにのみファイアウォールルールが適用されます。

        * **Target tags:**  
        NFSのインスタンス (ノード) で設定したネットワークタグを入力します。

        * **Source filter:**  
        **IP ranges**を選択する。  
        フィルタを設定して、特定のトラフィックソースにルールを適用します。

        * **Source IP ranges:**  
        アクセスしようとするIP rangesをCIDR表記で追加します。  
        例)10.0.10.0/24 192.168.100.5/32

        * **Protocols and ports:**  
        許可されているプロトコルおよびポートのみのトラフィック規則が適用されます。

            * Specified protocols and ports

                * **tcp:** 22

    ![gke-create-firewall-rules-ssh]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-firewall-rules-ssh.png)

* **NFS ファイアウォールルールを追加する。**

    * **Name:**  
    例) cocktail-test-gke-demo-storage-nfs

    * **Network:**  
    インスタンスで設定したVPCを選択します。

    * **Targets(対象):**  
    **Specified target tags**を選択する。  
    仮想ネットワークでは、インスタンスにのみファイアウォールルールが適用されます。

        * **Target tags:**  
        NFSのインスタンス (ノード) で設定したネットワークタグを入力します。

        * **Source filter:**  
        **Subnets**を選択する。 
        フィルタを設定して、特定のトラフィックソースにルールを適用します。

        * **Subnets:**  
        このソースサブネットワークのトラフィックのみが許可されます。 

        * **Protocols and ports:**  
        許可されているプロトコルおよびポートのみのトラフィック規則が適用されます。

            * Allow all 選択
            ここでは便宜上、Allow allを選択します。

    ![gke-create-firewall-rules-nfs]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-firewall-rules-nfs.png)

### 3. 生成されたインスタンス (ノード) へSSHで接続します。

* インスタンス画面で生成されたインスタンス (ノード) の **SSH**項目を選択 -> **View gcloud command**を選択する(自動生成)。

* **Run IN CLOUD SHELL**を実行する。

    * RSA key pair が自動生成される。
    次のコマンドで生成された key pair を確認することができる。

        ```
        cd ~/.ssh
        ```

    * RSA key pair GCP の Compute Engine > Metadata > SSH Keys に自動登録される。  
    下の図のように登録されたSSH Keyを確認、編集することができる。

![gke-create-connect-node-ssh-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-1.png)

* または RSA key pair を手動で作成/登録する方法

    * 次のコマンドで RSA key pair を生成する。

        ```
        ssh-keygen -t rsa -f ./rsa-gcp-key -C"<Your-email.com>"
        ```
    
    ![gke-create-connect-node-ssh-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-2.png)

    * 生成された RSA Key 内容をコピーして Compute Engine> Metadata> SSH Keysに登録する。

        ```
        cat ./rsa-gcp-key.pub
        ```

        ![gke-create-connect-node-ssh-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-3.png)
        ![gke-create-connect-node-ssh-4]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-4.png)


### 4. 生成されたインスタンス (ノード) にNFSサーバのインストール/構成

* ノードOSの確認。

    ```
    sudo grep . /etc/*-release
    ```

* ノードNFSパッケージを確認。

    ```
    sudo rpm -qa | grep nfs
    ```

* ノードNFSパッケージをインストール。

    ```
    sudo yum install nfs-utils nfs-utils-lib
    ```

* クラスタ（マスター）で、NFSに使用される共有ディレクトリを作成します。  
**provisioner Deployment**で **NFS_PATH**に指定されたディレクトリを作成します。

    ```
    sudo mkdir /storage/shared
    ```

* 共有ディレクトリパーミッション設定します。

    ```
    sudo chmod -R 777 /storage
    ```

* 共有ディレクトリ NFS のアクセス許可を付与し、同期する。 
インスタンス（ノード）で設定した **Subnet または IP ranges**を付与する。

    ```
    # sudo vi /etc/exports

    /storage/shared 10.0.10.0/24(rw,sync)
    ```

* NFS service を再起動する。

    ```
    sudo service nfs restart
    ```

### 5. クラスタ（マスター）に NFS-Client Provisioner 構成  
**参照:** [Kubernetes NFS-Client Provisioner](https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client)

* **Setup authorization:**  
クラスタにRBACが有効になっているか、OpenShiftを実行している場合、プロバイダに権限を付与する必要があります。

    * **kubectl apply -f 1-rbac.yaml**  
    Namespace、 ServiceAccount、 Setup authorization 適用

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
    name: cocktail-addon
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: nfs-client-provisioner
    namespace: cocktail-addon
    ---
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: nfs-client-provisioner-runner
    rules:
    - apiGroups: [""]
        resources: ["persistentvolumes"]
        verbs: ["get", "list", "watch", "create", "delete"]
    - apiGroups: [""]
        resources: ["persistentvolumeclaims"]
        verbs: ["get", "list", "watch", "update"]
    - apiGroups: ["storage.k8s.io"]
        resources: ["storageclasses"]
        verbs: ["get", "list", "watch"]
    - apiGroups: [""]
        resources: ["events"]
        verbs: ["list", "watch", "create", "update", "patch"]

    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: run-nfs-client-provisioner
    subjects:
    - kind: ServiceAccount
        name: nfs-client-provisioner
        # replace with namespace where provisioner is deployed
        namespace: cocktail-addon
    roleRef:
    kind: ClusterRole
    name: nfs-client-provisioner-runner
    apiGroup: rbac.authorization.k8s.io
    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: leader-locking-nfs-client-provisioner
    rules:
    - apiGroups: [""]
        resources: ["endpoints"]
        verbs: ["get", "list", "watch", "create", "update", "patch"]
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: leader-locking-nfs-client-provisioner
    subjects:
    - kind: ServiceAccount
        name: nfs-client-provisioner
        # replace with namespace where provisioner is deployed
        namespace: cocktail-addon
    roleRef:
    kind: Role
    name: leader-locking-nfs-client-provisioner
    apiGroup: rbac.authorization.k8s.io
    ```

* **Configure the NFS-Client provisioner**

    * **kubectl apply -f 2-shared-storage-provisioner-dp.yaml**  
    Deployment、次のプロビジョナー配布ファイルを編集して、NFSサーバへの接続情報を追加する必要があります。

    ```yaml
    kind: Deployment
    apiVersion: apps/v1
    metadata:
    name: shared-storage-provisioner
    namespace: cocktail-addon
    spec:
    replicas: 1
    selector:
        matchLabels:
        acornsoft.io/provisioner-type: NFSDYNAMIC
        app: shared-storage-provisioner
    strategy:
        type: Recreate
    template:
        metadata:
        labels:
            app: shared-storage-provisioner
            acornsoft.io/provisioner-type: NFSDYNAMIC
        spec:
        serviceAccount: nfs-client-provisioner
        containers:
            - name: shared-storage-provisioner
            image: quay.io/external_storage/nfs-client-provisioner:v2.0.1
            volumeMounts:
                - name: nfs-client-root
                mountPath: /persistentvolumes
            env:
                - name: PROVISIONER_NAME
                # YOUR PROVISIONER_NAME
                value: acornsoft.io/shared-storage-provisioner
                - name: NFS_SERVER
                # YOUR NFS SERVER HOSTNAME
                value: 10.0.10.8
                - name: NFS_PATH
                value: /storage/shared
            resources:
                limits:
                cpu: 100m
                memory: 128Mi
                requests:
                cpu: 100m
                memory: 128Mi
        tolerations:
        - key: node-role.kubernetes.io/master
            effect: NoSchedule
        volumes:
            - name: nfs-client-root
            nfs:
                # YOUR NFS SERVER HOSTNAME
                server: 10.0.10.8
                path: /storage/shared

    ```

* **Storage class 登録**  
参照: <https://kubernetes.io/docs/concepts/storage/storage-classes/>

    * **kubectl apply -f 3-single-sc.yaml**  
    single-storage : Persistent Volumes with Kubernetes on GKE

    ```yaml
    ---
    # single-storage : Persistent Volumes with Kubernetes on GKE
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
    name: single-storage
    annotations:
        storageclass.beta.kubernetes.io/is-default-class: "false"
    labels:
        acornsoft.io/provisioner-type: GCE
        acornsoft.io/type: SINGLE
        addonmanager.kubernetes.io/mode: EnsureExists
        kubernetes.io/cluster-service: "true"
    parameters:
    type: pd-standard
    provisioner: kubernetes.io/gce-pd
    reclaimPolicy: Delete
    ```

    * **kubectl apply -f 4-shared-sc.yaml**  
    shared-storage : NFS Persistent Volumes with Kubernetes on GKE

    ```yaml
    ---
    # shared-storage : NFS Persistent Volumes with Kubernetes on GKE
    apiVersion: storage.k8s.io/v1beta1
    kind: StorageClass
    metadata:
    name: shared-storage
    annotations:
        storageclass.beta.kubernetes.io/is-default-class: "false"
    labels:
        acornsoft.io/provisioner-type: NFSDYNAMIC
        acornsoft.io/total-capacity: "100"
        acornsoft.io/type: SHARED
    provisioner: acornsoft.io/shared-storage-provisioner
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    ```

### 6. サンプルPVC登録/確認。

* 以下のコマンドでサンプルを構成することができる。

    * **kubectl apply -f 5-test-pod.yaml**  
    一時ファイルの作成podを生成する。

    ```yaml
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
    name: nfs-pvc
    spec:
    storageClassName: shared-storage
    accessModes:
    - ReadWriteMany
    resources:
        requests:
        storage: 1Mi
    ---
    kind: Pod
    apiVersion: v1
    metadata:
    name: test-pod
    spec:
    containers:
    - name: test-pod
        image: gcr.io/google_containers/busybox:1.24
        command:
        - "/bin/sh"
        args:
        - "-c"
        - "touch /mnt/SUCCESS && exit 0 || exit 1"
        volumeMounts:
        - name: nfs-test-pvc
            mountPath: "/mnt"
    restartPolicy: "Never"
    volumes:
        - name: nfs-test-pvc
        persistentVolumeClaim:
            claimName: nfs-pvc
    ```

    * **kubectl get pv**  
    生成されたPVを確認する。

    * **NFS ノード**に生成された一時ファイルを確認することができる
