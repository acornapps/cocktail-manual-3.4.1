---
title: "NFS HA(High Availability) 構成"
excerpt: ""
permalink: /docs/ja/8.1.8/
redirect_from:
  - /theme-setup/
toc: false
toc_sticky: false
---

# Overview

Google Cloud 環境で Storage High Availibityのため、NFS serverを pacemaker、 corosync、 drbdを使って二重化する構成を説明します。
 - Corosync: クラスタのローレベルインフラを提供し、クラスタについてのコラム情報、メンバーシップ、安定的なメッセージングを提供
 - Pacemaker: クラスタリソース管理者。クラスターステータスによるリソース移動、ノード停止などを担当する。
 - DRBD(Distributed Replicated Block Device): ホスト間ネットワークを利用し、ソフトウェア的にblock deviceの contentを replicateする機能を提供する。

----

# Installation

### Google cloud VM instance 生成
* インスタンステンプレートを生成する。GCPインスタンスグループを生成する。
* Load balancer を生成する。 (ex: 10.174.0.4 )
* Load balancer を設定する。
    - Backend configuration  : 生成したインスタンスグループ選択後 2049ポート health check
    - Frontend configuration : PORT設定 all
    - ファイアウォールを設定する。

### NFS 1,2サーバーにDRBM設置
* DRBD package 設置

```
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
yum install -y kmod-drbd84 drbd84-utils
```

* Firewall off

```
systemctl stop firewalld
systemctl disable firewalld
```

* drbd configuration file 設定

```
mv /etc/drbd.d/global_common.conf /etc/drbd.d/global_common.conf.orig
vi /etc/drbd.d/global_common.conf

global {
 usage-count  yes;
}

common {
 net {
  protocol C;
 }
}
```

* lvm2 設置 pv 設定及びリソース設定

```
yum install -y lvm2 telnet

pvcreate /dev/sdb
vgcreate drbd_vg /dev/sdb
lvcreate -l 100%FREE -n drbd_lv drbd_vg
```

* 以下の conf設定時、gcpは hostnameにて変更 (hostname または、IP変更)

```
vi /etc/drbd.d/r0.res
resource r0 {
    handlers {
        split-brain "/usr/lib/drbd/notify-split-brain.sh root"; // alarm notiのための script file 設定
    }
    on nfs-master {
        device /dev/drbd0;
        disk /dev/drbd_vg/drbd_lv;
        meta-disk internal;
        address 10.128.0.18:7789;
    }
    on nfs-slave  {
        device /dev/drbd0;
        disk /dev/drbd_vg/drbd_lv;
        meta-disk internal;
        address 10.128.0.19:7789;
    }
}
```

* drbd リソース生成
```
drbdadm create-md r0
```

* drbd リソース開始
```
drbdadm up r0
```

* マスター昇格及びデバイス設定
```
drbdadm primary --force r0
mkfs.xfs /dev/drbd0
```

* システム enable
```
systemctl enable drbd
```

* NFS #1 サーバテスト
```bash
mount /dev/drbd0 /mnt
touch /mnt/test-01.txt
ls /mnt
umount /mnt
```

* NFS #2 サーバテスト    
```bash
// 1番でprimaryをsecondary設定
drbdadm secondary r0
```

* NFS #2で secondary を primary 設定
```bash
drbdadm primary r0
mount /dev/drbd0 /mnt
ls /mnt
umount /mnt
drbdadm secondary r0
```

* 現状の確認ができる
  - ブロックデバイスのシンク情報を確認することができる。
    ```    
    watch more /proc/drbd
    ```    
  - pacemaker ステータス確認
    ```    
    watch pcs status
    ```    

---

### NFS 1,2 サーバに pacemaker, corosync, pcsd 設置
* pacemaker 関連モジュール設置

```bash
yum install -y pacemaker pcs psmisc policycoreutils-python

passwd hacluster
systemctl start pcsd

setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

vi /etc/exports
// 接続帯域(CIDR)の定義が必要
/storage 10.128.0.0/24(rw,async,no_root_squash,no_all_squash,no_subtree_check)
```

* NFS#1(Master サーバで実行)

```bash
pcs cluster auth storage1 storage2
hacluster

pcs cluster setup --start --name nfs-cluster storage1 storage2 --force

// drdbコントロール設定部分
pcs cluster cib drbd_cfg
pcs -f drbd_cfg resource create Data ocf:linbit:drbd drbd_resource=r0 op monitor timeout="10" interval="10" role="Slave" op monitor timeout="10" interval="5" role="Master"
pcs -f drbd_cfg resource create Storage Filesystem device="/dev/drbd0" directory="/storage" fstype="xfs" op monitor interval="30s"
pcs -f drbd_cfg resource master DataSync Data master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
pcs -f drbd_cfg resource master DataSync NFS-Server master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
pcs -f drbd_cfg resource create NFS-Server systemd:nfs-server op monitor interval="40s"
pcs -f drbd_cfg resource group add HA-GROUP Storage  NFS-Server
pcs -f drbd_cfg constraint colocation add HA-GROUP DataSync INFINITY with-rsc-role="Master"
pcs -f drbd_cfg constraint colocation add HA-GROUP NFS-Server INFINITY with-rsc-role="Master"
pcs -f drbd_cfg constraint order promote DataSync then start Storage
pcs -f drbd_cfg constraint order Storage then start NFS-Server
pcs cluster cib-push drbd_cfg

//アクティブ化
pcs property set stonith-enabled=false
```

* デフォルトで pacemaker cluster heartbeat timeoutは 1秒になっているので、以下のように10秒に変更する。

```bash
# cat /etc/corosync/corosync.conf
totem {
    version: 2
    cluster_name: pcmk
    secauth: off
    transport: udpu
    token: 10000     <---- change totem timeout to 10s.
}

pcs cluster sync            // sync config file to all node
pcs cluster reload corosync // take effect all nodes without downtime
```

* 各 NFS serverで自動起動設定
```bash
systemctl enable corosync
systemctl enable pcsd
systemctl enable pacemaker
```

* pacemaker 起動/中止
```bash
systemctl start pcsd    // NFS1, NFS2を同時に起動
systemctl cluster [start | stop] // 片方のNFSを起動または終了
```

* pcsd web UI

https://{nfs server public ip}:2224

---

### 使用中に NFS storage disk size 増加
* GCE DISK 管理画面で NFSサーバに接続する 1,2 ディスクを同じサイズ程度増やす。 (ex: 50G -> 200G)
* NFS サーバで物理的なディスクサイズが増えたのかを確認
* DISK 論理構成の適用

```bash
partprobe
pvresize /dev/sdb
```

* 確認 1
```bash
pvdisplay
```

* 確認 2
```bash
lvscan
```

* ディスク拡張以前ステータス、以下のように切替可能
```bash
lvchange -a y /dev/drbd_vg/drbd_lv
lvresize -L +150G /dev/drbd_vg/drbd_lv
```

* ディスク拡張変更事項
```bash
lvextend -l +100%FREE /dev/drbd_vg/drbd_lv
```

* ディスク拡張後、適用することで df -hが適用できる。
```bash
xfs_growfs /dev/drbd0
lvscan
```

* マスターサーバーでシンク及び論理的なディスク適用
```bash
drbdadm resize r0
```

---
# Operations
### 1. DRBD ステータスを照会
```bash
drbdsetup show --show r0                    // 構成情報の照会
drbdadm status r0                           // ステータスの照会
drbdsetup  status r0 --verbose --statistic  // ステータス詳細の照会
drbdsetup events2 --now r0                  // リアルタイムイベントの照会
drbdadm cstate r0                           // ネットワークステータスの照会
drbdadm role r0                             // リソースの役割照会
drbdadm dstate r0                           // ディスクステータスの照会
```

### 2. Pacemaker ステータスを照会
```bash
pcs status                                 // pacemaker clusterステータスの照会
pcs cluster sync                           // リモートノードと構成の同期化
pcs cluster reload corosync                // 構成情報のリロード。 瞬断なし。
pcs alert config                           // Alert構成情報の照会
pcs cluster corosync [node]                // corosync.conf構成情報の照会

systemctl [start| stop] pcsd                // pcsd daemon start/stop
pcs cluster [start|stop]                    // pacemaker cluster start/stop

```

---
# Troubleshooting
### 1. NFS storage DRBD 接続が切れてから両ノードが Primary役割になっている場合、手動で復旧する方法
* DRBDが発生する alarmにおいて、split brainの発生を通知するalarmを発砲できる。
* この場合、NFS serverのネットワーク接続が切れてから両ノードがPrimary状態であることを感知し、まず変更事項を廃棄するnode(victim)を選択し、次のコマンドで手動復旧する。

```bash
drbdadm secondary r0
drbdadm connect --discard-my-data r0
```

他のノード(survivorと呼ぶ)では、接続コマンドを実行するのみ。

```bash
drbdadm disconnect r0    // まず、disconnectしてから、接続コマンドを遂行する。
drbdadm connect r0
```

正常に接続できたら、Split Brain victimの接続ステータスは直ちに SyncTargetに変更され、victimノードの変更事項は survivor ノードにより上書きされる。
次のコマンドで確認する。
```bash
drbdadm status r0
```
